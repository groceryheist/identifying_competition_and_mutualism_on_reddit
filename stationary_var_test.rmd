.---
title: "Testing stationarity enforcing priors for VAR models in Stan"
author: "Nate TeBlunthuis"
date: "June 25, 2020"
output:
 html_document:
  fit_width: 10
  fig_height: 7
---

In this notebook I'm testing out [Sarah Heap's method](https://arxiv.org/abs/2004.09455) for fitting var models using priors to enforce stationarity. 
Stationarity is important to VAR modeling because a non-stationary system will diverge: each variable will go to negative infinity or infinity.  A VAR model can represent non-stationary systems, but non-stationarity creates problems for estimation.  It's often claimed that "you can't (or shouldn't) fit a VAR model to non-stationary data, but this depends on the goals of analysis.  Problems with non-stationarity are more serious in the frequentist framework because the asymptotics depend on stationarity, non-stationary data can lead to 'spurious regerssions' with bias and incorrect standard errors.  

In a Bayesian framework this issue is less important, one simply doesn't have to worry about whether non-stationary data will mislead your sampler.  An issue with Bayesian var models though is that a sampler can simulate non-stationary systems, but these will diverge leading to divergent transitions in Stan.  This isn't much of a problem when the lags or the dimensionality of the system are low because your chances of drawing parameters from outside the stationary zone is not very high.  But the complexity of the system increases the non-stationary region grows faster than the stationary region leading to slow sampling and divergent transitions.  So from a practical perspective assuming stationarity should help stan sample *faster* and *avoid divergent transitions*.  

Is assuming stationarity realistic? This depends on the data, but in many cases it is realistic since it essentially just assumes that you won't forecast that the world will explode. The restrictions of the VAR model are already strict compared to the addition of a stationarity assumption.  We'll try not to reach beyond contexts where the stationarity assumption seems plausible. 

Heap's code fits var models to stationary space. Let's get started by simulating a stationary gaussian var system and then see if we can fit it using her code.  Heap's approach also supports sparse matricies. 


```{python, cache=FALSE}
##
from util import *

os.sched_setaffinity(0,range(os.cpu_count())) # I did this on the int-machine and need this line to use multiple cores

K = 3
N=1000
forecast_len=20

# i set growth_var to 0 since heap's model assumes 0 mean.
alpha, beta, sigma, _ = unpickle_or_create("simulation_data/gaussian_var_k3.pkl",
                                        overwrite=True,
                                        growth=0,
                                        growth_var=0,
                                        noise=1,
                                        function=gen_system,
                                        K=K,
                                        N=N)


y, df = evolve_var_system(alpha,beta,sigma,alpha,N,forecast_len)

data = {'m':K,
        'p':1,
        'N':N,
        'y':y,
        'es' : [0,0], # top-level prior for the means of the means (diag, off-diag)
        'fs' : [np.sqrt(0.7), np.sqrt(0.7)], # top-level prior for the precision of means. a pretty tight prior seems to help avoid multimodality
        'gs' : [2.1,2.1], # top-level prior for the position of the scales,
        'hs' : [1/3,1/3], # top-level prior for the precision of the scales,
        'scale_diag':1, # diagonal elements of the inverse wishart prior on the scale matrix
        'scale_offdiag':0.2, # off-diagonal elements of the inverse wishart prior on the scale matrix
        'df':10,
        'forecast_len':forecast_len} # degrees of freedom in the inverse wishart prior on the scale matrix.

heaps_model = unpickle_or_create("stan_code/heaps_statprior.pkl",
                                 overwrite=False,
                                 function=pystan.StanModel,
                                 file='stan_code/heaps_statprior.stan',
                                 model_name='heaps_statprior')

        
test_heaps_fit = unpickle_or_create("stan_models/var_1_heaps_3.pkl",
                                    overwrite=False,
                                    function=heaps_model.sampling,
                                    iter=2000,
                                    chains=4,
                                    data=data)

### it fits with no warnings!

assert(all(pystan.check_hmc_diagnostics(test_heaps_fit)))

df_pred = stan_var_predict(test_heaps_fit,N)
p = ggplot(df_pred,aes(y='y',ymax='y_upper',ymin='y_lower', x='x',group='variable')) + geom_ribbon(alpha=0.5) + geom_line(aes(color='variable'), linetype='-')
p = p + geom_line(aes(y='y',x='x',color='variable',group='variable'), data=df, inherit_aes=False)
p = p + theme(legend_position = 'top')
p.draw()
p.save("plots/test_heaps_fit.png") 

phi = test_heaps_fit.extract('phi')['phi']

phi_hat = phi.mean(axis=0)
phi_upper = np.quantile(phi, 0.975, axis=0)
phi_lower = np.quantile(phi, 0.025, axis=0)

sig = np.sign(phi_upper) == np.sign(phi_lower)
phi_hat = pd.DataFrame(phi_hat[0])

pos = phi_hat > 0
neg = phi_hat < 0
```

With this prior it becomes possible to fit much larger systems.

```{python, cache=FALSE}

K = 15
N=1500
forecast_len=20

# i set growth_var to 0 since heap's model assumes 0 mean. 
alpha, beta, sigma = unpickle_or_create("simulation_data/gaussian_var_k15.pkl",
                                        overwrite=False,
                                        growth=0,
                                        growth_var=0,
                                        noise=1,
                                        function=gen_system,
                                        K=K,
                                        N=N)
y0 = alpha
y, df = evolve_var_system(alpha,beta,sigma,y0,N,forecast_len)

data = {'m':K,
        'p':1,
        'N':N,
        'y':y,
        'es' : [0,0], # top-level prior for the means of the means (diag, off-diag)
        'fs' : [np.sqrt(0.7), np.sqrt(0.3)], # top-level prior for the precision of means. a pretty tight prior seems to help avoid multimodality
        'gs' : [2.1,1.5], # top-level prior for the position of the scales,
        'hs' : [1/3,1/5], # top-level prior for the precision of the scales,
        'scale_diag':0.2, # diagonal elements of the inverse wishart prior on the scale matrix
        'scale_offdiag':0.02, # off-diagonal elements of the inverse wishart prior on the scale matrix
        'df':K+3,
        'forecast_len':forecast_len} # degrees of freedom in the inverse wishart prior on the scale matrix.
        

test_heaps_fit_2 = unpickle_or_create("stan_models/var_1_heaps_15.pkl",
                                    overwrite=True,
                                    function=heaps_model_means.sampling,
                                    iter=2000,
                                    chains=4,
                                    data=data,
                                    refresh=25)

### it fits with no warnings!
assert(all(pystan.check_hmc_diagnostics(test_heaps_fit_2)))

df_pred = stan_var_predict(test_heaps_fit_2,N)
p = ggplot(df_pred,aes(y='y',ymax='y_upper',ymin='y_lower', x='x',group='variable')) + geom_ribbon(alpha=0.5) + geom_line(aes(color='variable'), linetype='-')
p = p + geom_line(aes(y='y',x='x',color='variable',group='variable'), data=df, inherit_aes=False)
p = p + theme(legend_position = 'top')
p.draw()
p.save("plots/test_heaps_fit_15.png")

phi = test_heaps_fit_2.extract('phi')['phi']

phi_hat = phi.mean(axis=0)
phi_upper = np.quantile(phi, 0.975, axis=0)
phi_lower = np.quantile(phi, 0.025, axis=0)

sig = np.sign(phi_upper) == np.sign(phi_lower)
phi_hat = pd.DataFrame(phi_hat[0])

pos = phi_hat > 0
neg = phi_hat < 0


```

There are a few extensions to make for Heap's approach to be useful in my application. 

1. We want to generate forecasts. (Done)
2. We want to have seperate means instead of mean 0 processes. (Done and we can recover parameters for mu and phi and sigma.)
3. We want to fit on data that are poisson (Done) or negative binomial distributed.
4. Use a more numerically stable algorithm for computing the matrix square root. (might be a real challenge)
4. (Later) We want to encourage sparsity. 
5. (Later) We want to have trends.

I already added forecast generation to heap's program. I extended her stan program with a hyper prior on the means. 

```{python}

K = 3
N=1000
forecast_len=20

# i set growth_var to 0 since heap's model assumes 0 mean. 
alpha, beta, sigma = unpickle_or_create("simulation_data/gaussian_var_k3_nonzero.pkl",
                                        overwrite=False,
                                        growth=3,
                                        growth_var=1,
                                        noise=1,
                                        function=gen_system,
                                        K=K,
                                        N=N)

y0 = alpha
y, df = evolve_var_system(alpha,beta,sigma,y0,N,forecast_len)

data = {'m':K,
        'p':1,
        'N':N,
        'y':y,
        'es' : [0,0], # top-level prior for the means of the means (diag, off-diag)
        'fs' : [np.sqrt(0.7), np.sqrt(0.7)], # top-level prior for the precision of means. a pretty tight prior seems to help avoid multimodality
        'gs' : [2.1,2.1], # top-level prior for the position of the scales,
        'hs' : [1/3,1/3], # top-level prior for the precision of the scales,
        'scale_diag':0.5, # diagonal elements of the inverse wishart prior on the scale matrix
        'scale_offdiag':0.0, # off-diagonal elements of the inverse wishart prior on the scale matrix
        'df':10,
        'forecast_len':forecast_len,
        'alpha':0,  # hyper prior for mean of alpha
        'sd0': 5, # hyper prior precision of mu0
        'g0':4, # hyper prior for variance of m0
        'h0':3, # hyper prior for variance of m0
} # degrees of freedom in the inverse wishart prior on the scale matrix.

model_mu = unpickle_or_create("stan_code/heaps_statprior_means.pkl",
                                 overwrite=False,
                                 function=pystan.StanModel,
                                 file='stan_code/heaps_statprior_means.stan',
                                 model_name='heaps_statprior_means')

        

test_mu_fit = unpickle_or_create("stan_models/var_heaps_means_2.pkl",
                                 overwrite=True,
                                 function=model_mu.sampling,
                                 iter=2000,
                                 chains=4,
                                 data=data)

### it fits with no warnings!

assert(all(pystan.check_hmc_diagnostics(test_mu_fit)))

df_pred = stan_var_predict(test_mu_fit,N)
p = ggplot(df_pred,aes(y='y',ymax='y_upper',ymin='y_lower', x='x',group='variable')) + geom_ribbon(alpha=0.5) + geom_line(aes(color='variable'), linetype='-',size=2)
p = p + geom_line(aes(y='y',x='x',color='variable',group='variable'), data=df, inherit_aes=False)
p = p + xlim(800,1020)
p = p + theme(legend_position = 'top')
p.draw()
p.save("plots/test_heaps_fit_means.png")

mu = test_mu_fit.extract('mu')['mu']

mu_hat = mu.mean(axis=0)

phi = test_mu_fit.extract('phi')['phi']

phi_hat = phi.mean(axis=0)
phi_upper = np.quantile(phi, 0.975, axis=0)
phi_lower = np.quantile(phi, 0.025, axis=0)

sigma_hat = test_mu_fit.extract('Sigma')['Sigma'].mean(axis=0)

sig = np.sign(phi_upper) == np.sign(phi_lower)
phi_hat = pd.DataFrame(phi_hat[0])

```

Now let's see if we can do it with a poisson link.

I was having some trouble figuring out a model structure that would work best.  I was confused for some time by a bug. 
There seem to be two options that can work: 

1. Have Heap's model as a latent gaussian model for the $log(\lambda)$ in the poisson model. In this we multiply parmaeter $\phi$ by parameter $\lambda$. Multiplying parameters can lead to sampling difficulties.

2. Follow Brandt and Sandler's approach with the var process in non-exponentiated space. This is more complicated and might break Heap's model (not 100% sure about this).  The issue is that this model doesn't have a model for the covariance of the errors, which is a second drawback: the model assumes heterogeneity. The main advantage is that we can avoid multiplying parameters.  This model also has fewer latent parameters. 

I'm going with option 1 for now. There might be some sampling difficulties and divergent transitions on the real data.  Code for option 2 is in `heaps\_poisson\__nonexp.stan`.


```{python}
####

K = 3
N = 400
forecast_len = 20

# i set growth_var to 0 since heap's model assumes 0 mean. 
alpha, beta, sigma = unpickle_or_create("simulation_data/gaussian_var_k3_n400_nonzero.pkl",
                                        overwrite=True,
                                        growth=3,
                                        growth_var=1,
                                        community=0.3,
                                        community_var=0.1,
                                        noise=0.2,
                                        function=gen_system,
                                        K=K,
                                        N=N)


y, df = evolve_var_system(alpha,beta,sigma,alpha,N,forecast_len,link=lambda x: np.random.poisson(np.exp(x)),nested_alpha=False)

data = {'m':K,
        'p':1,
        'N':N,
        'y':y.T,
        'es' : [0,0], # top-level prior for the means of the means (diag, off-diag)
        'fs' : [np.sqrt(0.7), np.sqrt(0.7)], # top-level prior for the precision of means. a pretty tight prior seems to help avoid multimodality
        'gs' : [2.1,2.1], # top-level prior for the position of the scales,
        'hs' : [1/3,1/3], # top-level prior for the precision of the scales,
        'scale_diag':0.5, # diagonal elements of the inverse wishart prior on the scale matrix
        'scale_offdiag':0.0, # off-diagonal elements of the inverse wishart prior on the scale matrix
        'df':10,
        'forecast_len':forecast_len,
        'alpha':0,  # hyper prior for mean of alpha
        'sd0': 5, # hyper prior precision of mu0
        'g0':4, # hyper prior for variance of m0
        'h0':3, # hyper prior for variance of m0
} # degrees of freedom in the inverse wishart prior on the scale matrix.


model_pois_noncentered = unpickle_or_create("stan_code/heaps_poisson_noncentered.pkl",
                                         overwrite=False,
                                         function=pystan.StanModel,
                                         file='stan_code/heaps_poisson_noncentered.stan',
                                         model_name='heaps_statprior_pois_noncentered')

test_pois_fit_noncentered = unpickle_or_create("stan_models/var_heaps_pois_3_latent_noncentered_n400.pkl",
                                               overwrite=True,
                                               function=model_pois_noncentered.sampling,
                                               iter=2000,
                                               chains=4,
                                               data=data,
                                               control={'adapt_delta':0.92} # jacking up adapt_delta removes divergences
)


### it fits with no warnings!
assert(all(pystan.check_hmc_diagnostics(test_pois_fit_noncentered)))

df_pred = stan_pois_var_predict(test_pois_fit_noncentered,N)
p = ggplot(df_pred,aes(y='y',ymax='y_upper',ymin='y_lower', x='x',group='variable')) + geom_ribbon(alpha=0.5) + geom_line(aes(color='variable'), linetype='-',size=2)
p = p + geom_line(aes(y='y',x='x',color='variable',group='variable'), data=df, inherit_aes=False)

p = p + theme(legend_position = 'top')
p.draw()
p = p + theme(plot_background=element_rect(fill='white'))
p = p + scale_y_log10()
p.save("plots/test_heaps_fit_pois_noncentered.png")

mu = test_pois_fit_noncentered.extract('mu')['mu']

mu_hat = mu.mean(axis=0)
phi = test_pois_fit_noncentered.extract('phi')['phi']
phi_hat = phi.mean(axis=0)
phi_upper = np.quantile(phi, 0.975, axis=0)
phi_lower = np.quantile(phi, 0.025, axis=0)

sigma_hat = test_mu_fit.extract('Sigma')['Sigma'].mean(axis=0)

sig = np.sign(phi_upper) == np.sign(phi_lower)
phi_hat = pd.DataFrame(phi_hat[0])
```

Now that we've got the Poisson model working let's do the negative binomial. 

```{python,warning=F}
K = 3
N = 400
forecast_len=100
alpha, beta, sigma = unpickle_or_create("simulation_data/var_1_sim_3_negbin.pkl",
                                        overwrite=True,
                                        function=gen_system,
                                        growth=3.1,
                                        growth_var=0.5,
                                        community=0,
                                        community_var=2,
                                        K=K,
                                        N=N,
                                        noise=0.4,
                                        seed=1234)
y0 = alpha

# we need overdispersion parameters
phi = np.abs(np.random.normal(0,1,K))
thetas = 1/np.sqrt(phi)

link = lambda mu, theta: np.random.negative_binomial(* convert_negbin_params(np.exp(mu), theta))

y, df = evolve_var_system(alpha, beta, sigma, y0, N, forecast_len, link_args=[thetas], link = link, nested_alpha=False)

data = {'m':K,
        'p':1,
        'N':N,
        'y':y.T,
        'es' : [0,0], # top-level prior for the means of the means (diag, off-diag)
        'fs' : [np.sqrt(0.3), np.sqrt(0.3)], # top-level prior for the precision of means. a pretty tight prior seems to help avoid multimodality
        'gs' : [1/2,1/2], # top-level prior for the position of the scales,
        'hs' : [1/2,1/2], # top-level prior for the precision of the scales,
        'scale_diag':0.5, # diagonal elements of the inverse wishart prior on the scale matrix
        'scale_offdiag':0.1, # off-diagonal elements of the inverse wishart prior on the scale matrix
        'df':8,
        'forecast_len':forecast_len,
        'alpha':0,  # hyper prior for mean of alpha
        'sd0': 5, # hyper prior precision of mu0
        'g0':4, # hyper prior for variance of m0
        'h0':3, # hyper prior for variance of m0
        'ts0':0, # hyper prior for variance of theta (overdispersion parameter)
        'tss':5, # hyper prior for variance of theta (overdispersion parameter)
        'tm0':0, # hyper prior for mean of theta (overdispersion parameter)
        'tms':2, # hyper prior for mean of theta (overdispersion parameter)
        
} # degrees of freedom in the inverse wishart prior on the scale matrix.


heaps_negbin_mod_noncentered = unpickle_or_create("stan_code/heaps_negbin_noncentered.pkl",
                                      overwrite=True,
                                      function=pystan.StanModel,
                                      file="stan_code/heaps_negbin_noncentered.stan",
                                      model_name='heaps_negbin_noncentered'
)

heaps_negbin_mod = unpickle_or_create("stan_code/heaps_negbin.pkl",
                                      overwrite=False,
                                      function=pystan.StanModel,
                                      file="stan_code/heaps_negbin.stan",
                                      model_name='heaps_negbin_noncentered'
)


# The centered parameterization has divergences and low bfmi
heaps_negbin_fit = unpickle_or_create("stan_models/heaps_negbin_3.pkl",
                                      overwrite=True,
                                      function=heaps_negbin_mod.sampling,
                                      data=data,
                                      iter=2000,
                                      control={'adapt_delta':0.95,
                                               'max_treedepth':12})

heaps_negbin_noncentered_fit = unpickle_or_create("stan_models/heaps_negbin_3.pkl",
                                                 overwrite=True,
                                                 function=heaps_negbin_mod_noncentered.sampling,
                                                 data=data,
                                                 iter=2000,
                                                 control={'adapt_delta':0.99,
                                                          'max_treedepth':12})

df_pred = stan_pois_var_predict(heaps_negbin_noncentered_fit, N)
#heaps_negbin_fit = heaps_poisson_fit_on_negbin
#df_pred = stan_negbin_var_predict(heaps_negbin_fit, N,mean_name='lambda_new')

p = ggplot(df_pred,aes(y='y',ymax='y_upper',ymin='y_lower', x='x',group='variable')) + geom_ribbon(alpha=0.5) + geom_line(aes(color='variable'), linetype='-',size=2)
p = p + geom_line(aes(y='y',x='x',color='variable',group='variable'), data=df, inherit_aes=False)

p = p + theme(legend_position = 'top')

p.draw()
p = p + theme(plot_background=element_rect(fill='white'))
p.save("plots/negbin_3_noncentered_pred.pdf",width=12,height=10)

mu = heaps_negbin_fit.extract('mu')['mu']

mu_hat = mu.mean(axis=0)

phi = heaps_negbin_fit.extract('phi')['phi']
phi_hat = phi.mean(axis=0)
phi_upper = np.quantile(phi, 0.975, axis=0)
phi_lower = np.quantile(phi, 0.025, axis=0)

sigma_hat = heaps_negbin_fit.extract('Sigma')['Sigma'].mean(axis=0)
theta_hat = heaps_negbin_fit.extract('theta')['theta'].mean(axis=0)

# check the pairs plot_ar

#df = heaps_negbin_fit.to_dataframe(inc_warmup=True,diagnostics=True,pars=['A','mu','theta','Sigma'])
df = heaps_negbin_noncentered_fit.to_dataframe(inc_warmup=True,diagnostics=True,pars=['A','mu','Sigma','theta'])
pairsdata = df.melt(id_vars=['energy__','divergent__'])
#pairsdata['xdraw'] = (pairsdata.draw.astype(str) + '0' + pairsdata.chain.astype(str)).astype(int)

p = ggplot(pairsdata, aes(x='energy__',y='value',color='divergent__')) + geom_point(alpha=0.5) + facet_wrap(('variable'),ncol=3,scales='free')
p = p + theme(plot_background=element_rect(fill='white'))
p.save('plots/negbin_noncentered_pairs.png',width=12,height=150,limitsize=False)

```

So even with the stationarity enforcing prior, the negative binomial models still seem to struggle. I spent a lot of time experimenting with and thinking about the Poisson and Negative binomial models.  For a time I thought that stronger priors would be the solution, and while that can help that doesn't give me much confidence about being able to scale the analysis on real data. I tried out non-centered parameterizations. This can help in some situations, but  doesn't solve the underlying multimodality problem inherent in the model.

Both of the models struggle with identifiability issues that I believe are linked to dependence between the variance terms in the link and the variance-covariance matrix.  My intuition is that there's simply too many places for noise to go. It can show up in the covariance matrix and in the overdispersion parameter. So it's going to have a hard time splitting between the errors and the negative binomial. 

In the case of the Poisson model (though things are clearely worse in the case of the negbin), the model (ignoring priors) is: 

$Y_t \sim Pois(\lambda_t)$

$\lambda_t \sim \mathrm(MVN)(\mu_t, \Sigma) + \alpha$

$\mu_t = A mu_{t-1}$

Since this is a poisson model the maringal mean and variance of the outcome are:

$E[Y_t] = VAR[Y_t] = \lambda_t$

So the variance depends on both $\Sigma$ and $A$. In the case of Poisson my intuition is that this might not be a huge problem since $\lambda_t$ can probably be identified by $\mu_t$.  The situation is worse for negative binomial models because

$\Y_t \sim NB(\lambda_t, \theta)$ 

$ E[Y_t] = \lambda_t$

$ VAR[Y_t] = \lambda_t + \frac{\lambda_t}{\theta} $

So now variance of $Y_t$ depends on $\Sigma$, $\theta$, and $A$. And $\Sigma$ and $\theta$ are both largely informed by the variance. This seems like a recipe for identifiability problems. 

Following an approach like Brandt and Sandler's proposal seems like it will help with this issue by getting rid of $\Sigma$. 
Getting rid of $\Sigma$ essentially assumes that there's no simultaneous covariance between the series, which seems dubious.  This change will almost certainly affect our predictive uncertainty, the overall shape of the posterior and might lead to new sampling problems.  It isn't obvious is this is better or worse than log-transforming count data, which introduces bias, but is commonly done anyway. 

From looking at the data I think it will be important to model trends. Probably a linear trend will suffice (the early period looks like exponential growth, but most of the time it just looks pretty linear.  So let's make a stan model 

```{python}


```{python, cache=FALSE}

K = 2
N= 200
forecast_len=20

# i set growth_var to 0 since heap's model assumes 0 mean. 
alpha, beta, sigma, slopes = unpickle_or_create("simulation_data/gaussian_var_k2_slopes.pkl",
                                        overwrite=False,
                                        growth=3,
                                        growth_var=4,
                                                community=0.7,
                                                community_var=0.2,
                                        noise=0.2,
                                        function=gen_system,
                                        K=K,
                                        N=N,
                                        time_slopes_mean = 1,
                                        time_slopes_var = 0.5
)

y0 = alpha
y, df = evolve_var_system(alpha,beta,sigma,y0,N,forecast_len,slopes)

data = {'m':K,
        'p':1,
        'N':N,
        'y':y,
        'es' : [0,0], # top-level prior for the means of the means (diag, off-diag)
        'fs' : [np.sqrt(10), np.sqrt(10)], # top-level prior for the precision of means. a pretty tight prior seems to help avoid multimodality
        'gs' : [2.1,1.5], # top-level prior for the position of the scales,
        'hs' : [1/3,1/5], # top-level prior for the precision of the scales,
        'scale_diag':0.2, # diagonal elements of the inverse wishart prior on the scale matrix
        'scale_offdiag':0.02, # off-diagonal elements of the inverse wishart prior on the scale matrix
        'df':K+3,
        'forecast_len':forecast_len,
        'alpha':0,  # hyper prior for mean of alpha
        'sd0': 5, # hyper prior precision of mu0
        'g0':4, # hyper prior for variance of m0
        'h0':3, # hyper prior for variance of m0
        'alpha_time':0.0,
        'sd_time':1,
        'g_time':0.5,
        'h_time':1
} # degrees of freedom in the inverse wishart prior on the scale matrix.
# 
  # real alpha_time;
  # real<lower=0> sd_time;
  # real<lower=0> g_time;
  # real<lower=0> h_time;

heaps_trend_mod = unpickle_or_create("stan_code/heaps_trend.pkl",
                                      overwrite=False,
                                      function=pystan.StanModel,
                                      file="stan_code/heaps_trend.stan",
                                      model_name='heaps_trend'
)


test_trend_fit = unpickle_or_create("stan_models/var_1_heaps_k2_slopes.pkl",
                                    overwrite=True,
                                    function=heaps_trend_mod.sampling,
                                    iter=2000,
                                    chains=4,
                                    data=data
)


test_mu_fit = model_mu.sampling(data=data)

# can we recover the means?

df = test_trend_fit.to_dataframe(inc_warmup=True,diagnostics=True,pars=['A','mu','Sigma','beta_time'])
pairsdata = df.melt(id_vars=['energy__','divergent__'])
#pairsdata['xdraw'] = (pairsdata.draw.astype(str) + '0' + pairsdata.chain.astype(str)).astype(int)

p = ggplot(pairsdata, aes(x='energy__',y='value',color='divergent__')) + geom_point(alpha=0.5) + facet_wrap(('variable'),ncol=3,scales='free')
p = p + theme(plot_background=element_rect(fill='white'))
p = p + scale_x_log10()
p.save('plots/trends_pairs.png',width=12,height=150,limitsize=False)


```


I'm having trouble with that. So I'm going to try working up from an even simpler model. Just an AR(1) model with a trend. This works fine:

```{python}

a = 0.5
b = 0.2
N = 400

y = [b]

def y_next(y, a, b,i):
    return y*a + b*(i+1) + np.random.normal(0,0.1)

for i in range(1,N):
    y.append(y_next(y[i-1], a, b,i))

ar1_trend_mod = unpickle_or_create("stan_code/ar_with_trend.pkl",
                                   overwrite=True,
                                   function=pystan.StanModel,
                                   file='stan_code/ar_with_trend.stan',
                                   model_name='ar_trend')

data = {'N':N,
        'Y':y}
                                   
test_ar1 = ar1_trend_mod.sampling(data=data)

```


The time trend is difficult to solve using a VAR(1) + time trend approach.  Probably the answer is just to have multiple lags.  Models with multiple lags can represent complex trajectories as long as they are smooth / stationary.  We should test out our poisson models with multiple lags. 

```{python}

K = 2
N = 800
forecast_len = 20
P = 2

# i set growth_var to 0 since heap's model assumes 0 mean. 
alpha, beta, sigma = unpickle_or_create("simulation_data/gaussian_var_k2_p2_nonzero.pkl",
                                        overwrite=True,
                                        growth=3,
                                        growth_var=1,
                                        community=[0.9,0.8],
                                        community_var=[0.1,0.1],
                                        noise=0.2,
                                        function=gen_system,
                                        K=K,
                                        N=N)

yinit = list(reversed([np.repeat(0,K), alpha]))
y, df = evolve_var_system(alpha,beta,sigma,yinit,N,forecast_len,nested_alpha=True)

data = {'m':K,
        'p':P,
        'N':N,
        'y':y,
        'es' : [0,0], # top-level prior for the means of the means (diag, off-diag)
        'fs' : [np.sqrt(1.7), np.sqrt(1.7)], # top-level prior for the precision of means. a pretty tight prior seems to help avoid multimodality
        'gs' : [2.1,2.1], # top-level prior for the position of the scales,
        'hs' : [1/3,1/3], # top-level prior for the precision of the scales,
        'scale_diag':0.5, # diagonal elements of the inverse wishart prior on the scale matrix
        'scale_offdiag':0.0, # off-diagonal elements of the inverse wishart prior on the scale matrix
        'df':10,
        'forecast_len':forecast_len,
        'alpha':0,  # hyper prior for mean of alpha
        'sd0': 5, # hyper prior precision of mu0
        'g0':4, # hyper prior for variance of m0
        'h0':3, # hyper prior for variance of m0
} # degrees of freedom in the inverse wishart prior on the scale matrix.


model_normal = unpickle_or_create("stan_code/heaps_statprior_means.pkl",
                                         overwrite=False,
                                         function=pystan.StanModel,
                                         file='stan_code/heaps_statprior_means.stan',
                                         model_name='heaps_statprior_means')

test_fit = unpickle_or_create("stan_models/var_heaps_p2_k2_n400.pkl",
                              overwrite=True,
                              function=model_normal.sampling,
                              iter=2000,
                              chains=4,
                              data=data,
                              control={'adapt_delta':0.92} # jacking up adapt_delta removes divergences
)

test_pois_fit = test_fit

df_pred = stan_pois_var_predict(test_pois_fit,N)

p = ggplot(df_pred,aes(y='y',ymax='y_upper',ymin='y_lower', x='x',group='variable')) + geom_ribbon(alpha=0.5) + geom_line(aes(color='variable'), linetype='-',size=2)

p = p + geom_line(aes(y='y',x='x',color='variable',group='variable'), data=df, inherit_aes=False)

p = p + theme(legend_position = 'top')
p.draw()
p = p + theme(plot_background=element_rect(fill='white'))
p = p + scale_y_log10()
p.save("plots/test_heaps_fit_pois_p3.png")

mu = test_pois_fit.extract('mu')['mu']

mu_hat = mu.mean(axis=0)
phi = test_pois_fit.extract('phi')['phi']
phi_hat = phi.mean(axis=(0))
phi_upper = np.quantile(phi, 0.975, axis=0)
phi_lower = np.quantile(phi, 0.025, axis=0)

sigma_hat = test_pois_fit.extract('Sigma')['Sigma'].mean(axis=0)

sig = np.sign(phi_upper) == np.sign(phi_lower)
phi_hat = pd.DataFrame(phi_hat[0])


test_mu_fit = model_mu.sampling(data=data)

# can we recover the means?

df = test_pois_fit.to_dataframe(inc_warmup=True,diagnostics=True,pars=['A','mu','Sigma','phi'])
pairsdata = df.melt(id_vars=['energy__','divergent__'])
#pairsdata['xdraw'] = (pairsdata.draw.astype(str) + '0' + pairsdata.chain.astype(str)).astype(int)

p = ggplot(pairsdata, aes(x='energy__',y='value',color='divergent__')) + geom_point(alpha=0.5) + facet_wrap(('variable'),ncol=3,scales='free')
p = p + theme(plot_background=element_rect(fill='white'))
p = p + scale_x_log10()
p.save('plots/pois_p6_pairs.png',width=12,height=80,limitsize=False)

```
